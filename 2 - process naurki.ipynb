{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71221e14-f1f4-42a8-b2cc-6fa783722155",
   "metadata": {},
   "source": [
    "## importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def import_or_install(package):\n",
    "    \"\"\"Try to import a package; if not available, install it.\"\"\"\n",
    "    try:\n",
    "        __import__(package)  # Try to import the package\n",
    "    except ImportError:\n",
    "        print(f\"{package} not installed. Installing...\")\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', package])\n",
    "        print(f\"{package} has been installed.\")\n",
    "    else:\n",
    "        print(f\"{package} is already installed.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy is already installed.\n",
      "selenium is already installed.\n",
      "bs4 is already installed.\n",
      "requests is already installed.\n",
      "lxml is already installed.\n",
      "pandas not installed. Installing...\n",
      "Collecting pandas\n",
      "  Using cached pandas-2.3.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (91 kB)\n",
      "Requirement already satisfied: numpy>=1.26.0 in ./.venv/lib/python3.12/site-packages (from pandas) (2.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Using cached pandas-2.3.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.0 MB)\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Installing collected packages: pytz, tzdata, pandas\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [pandas]━━━━━━━━━━━\u001b[0m \u001b[32m2/3\u001b[0m [pandas]\n",
      "\u001b[1A\u001b[2KSuccessfully installed pandas-2.3.1 pytz-2025.2 tzdata-2025.2\n",
      "pandas has been installed.\n"
     ]
    }
   ],
   "source": [
    "import_or_install('numpy')\n",
    "import_or_install('selenium')\n",
    "import_or_install('bs4')\n",
    "import_or_install('requests')\n",
    "import_or_install('lxml')\n",
    "import_or_install('pandas')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40155f4e-9a5c-442b-89c7-b30fc80f8ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np \n",
    "from bs4 import BeautifulSoup \n",
    "import requests\n",
    "from selenium import webdriver \n",
    "import time\n",
    "from selenium.webdriver.common.by import By \n",
    "import pandas as pd \n",
    "import random \n",
    "import json \n",
    "from collections import defaultdict\n",
    "import ast\n",
    "import datetime\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a957a5b-5bcb-40c5-888f-101c6dcbc3a1",
   "metadata": {},
   "source": [
    "## functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22eafff-94d3-4a69-bf29-dafa07343e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# manage json \n",
    "def write_json(data,fname,fpath=None):\n",
    "    if fpath:\n",
    "        loc = fpath + '/' + fname + '.json'\n",
    "    else: \n",
    "        loc = os.getcwd()+\"/\" + fname + '.json'\n",
    "\n",
    "    with open(loc, 'w') as json_file:\n",
    "        json.dump(data, json_file)\n",
    "\n",
    "\n",
    "def read_json(fname,fpath=None):\n",
    "    if fpath:\n",
    "        loc = fpath + '/' + fname + '.json'\n",
    "    else: \n",
    "        loc = os.getcwd()+\"/\" + fname + '.json'\n",
    "\n",
    "    try:\n",
    "        with open(loc, 'r') as json_file:\n",
    "            data = json.load(json_file)\n",
    "            return data\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file {loc} does not exist.\")\n",
    "        return {}  # Return an empty dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2a54e8-fd08-4d19-8a96-364f32aa0f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this functions scrolls till the end of the page\n",
    "\n",
    "def scroll(driver):\n",
    "    last_height = driver.execute_script(\"window.scrollTo(0,100);\")\n",
    "    \n",
    "    while True:\n",
    "        last_height = driver.execute_script(\"return window.scrollY\")\n",
    "        time.sleep(1)\n",
    "    \n",
    "        driver.execute_script(\"window.scrollBy(0,window.scrollY);\")\n",
    "        new_height = driver.execute_script(\"return window.scrollY\")\n",
    "        # print(f'new height = {new_height} , last_height = {last_height}')\n",
    "    \n",
    "        if new_height == last_height:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06f7633-1749-421e-9217-b9a33ec579aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function collects data from the tiles and accumulates in the tiles list\n",
    "\n",
    "def collect_data(driver,tile_class):\n",
    "    html_content = driver.page_source\n",
    "    soup = BeautifulSoup(html_content,'lxml')\n",
    "    tile = soup.find_all('div',attrs={\"class\":tile_class})\n",
    "    return tile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e782a80-6676-4b58-a554-ee210ac065a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random wait \n",
    "\n",
    "def wait():\n",
    "    wait = random.randint(1,5)\n",
    "    time.sleep(wait)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80404d5-ce22-4c42-958b-80396c3527b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finds the next button on naurki website and clicks it \n",
    "\n",
    "def click_next(driver,button_class):\n",
    "    buttons = driver.find_elements(By.CLASS_NAME,button_class)\n",
    "\n",
    "    if not buttons:\n",
    "        return 'stop'\n",
    "\n",
    "    next_button_found = 0\n",
    "    for button in buttons:\n",
    "        # print(button.text.lower())\n",
    "        if 'next' in button.text.lower():\n",
    "            next_button_found =1 \n",
    "\n",
    "            is_disabled = button.get_attribute(\"disabled\")\n",
    "            if is_disabled:\n",
    "                return 'stop'\n",
    "            else:\n",
    "                driver.execute_script(\"arguments[0].click()\",button)\n",
    "\n",
    "    if not next_button_found:\n",
    "        return 'stop'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bc4068-ad00-4823-876e-71ce95877ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert text file to bs4 element\n",
    "\n",
    "def file_to_list(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        # Read lines and strip newline characters\n",
    "        lines = [line.strip() for line in file.readlines()]\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23bd4a9-3930-4387-afef-5c4077f08383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract element data and convert it into a dictionary through which the data will be extracted via keys \n",
    "\n",
    "def extract_tag_data_to_dict_and_df(ele):\n",
    "    ele_data = defaultdict(list)\n",
    "\n",
    "    current_ele = ele\n",
    "    c = '0'\n",
    "    while current_ele is not None:\n",
    "        # Safely extract attributes and set defaults\n",
    "        c = current_ele.get('class', c) if hasattr(current_ele, 'get') else c\n",
    "        tag = current_ele.name if hasattr(current_ele, 'name') else '0'\n",
    "        \n",
    "        # Create the key tuple\n",
    "        key = (str(c),str(tag), 'text')\n",
    "\n",
    "        # Get the text, defaulting to empty string if None\n",
    "        text = current_ele.text if current_ele.text is not None else ''\n",
    "        \n",
    "        # Append the text to the list for the corresponding key\n",
    "        ele_data[key].append(text)\n",
    "\n",
    "        # Move to the next element\n",
    "        current_ele = current_ele.next_element\n",
    "\n",
    "    # Remove duplicates from the lists \n",
    "    for k, v in ele_data.items():\n",
    "        ele_data[k] = list(dict.fromkeys(v))\n",
    "\n",
    "    # Create a string representation of the text lists, joined by '--'\n",
    "    ele_data_str = {k: \"--\".join(str(item) for item in v if item) for k, v in ele_data.items()}\n",
    "    \n",
    "    # Convert the dictionary to a DataFrame\n",
    "    ele_data_df = pd.DataFrame.from_dict(ele_data_str, orient='index', columns=['text'])\n",
    "\n",
    "    return ele_data_str, ele_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute python file\n",
    "\n",
    "def execute_python_file(filepath):\n",
    "    with open(filepath, 'r') as file:\n",
    "        code = file.read()\n",
    "    exec(code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39435bdc-3914-4bf2-a78b-45a98b69be79",
   "metadata": {},
   "source": [
    "## initialise parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab07e262-b411-45f5-8cd5-b39c6279facf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting url\n",
    "# urls = read_json('n_urls')\n",
    "\n",
    "urls = {'testing':'https://www.naukri.com/jobs-in-india?&wfhType=2&jobAge=2&experience=22'}\n",
    "\n",
    "urls = {'naukri : all jobs + no_filter + remote + 2 days + 10+ years': 'https://www.naukri.com/jobs-in-india?&wfhType=2&jobAge=2&experience=10',\n",
    " 'naukri : all jobs + no_filter + remote + 2 days + 12+ years': 'https://www.naukri.com/jobs-in-india?&wfhType=2&jobAge=2&experience=12',\n",
    " 'naukri : all jobs + no_filter + Delhi - all areas + 2 days + 10+ years': 'https://www.naukri.com/jobs-in-india?cityTypeGid=6&cityTypeGid=73&cityTypeGid=220&cityTypeGid=9508&jobAge=2&experience=10',\n",
    " 'naukri : all jobs + no_filter + Delhi - all areas + 2 days + 12+ years': 'https://www.naukri.com/jobs-in-india?cityTypeGid=6&cityTypeGid=73&cityTypeGid=220&cityTypeGid=9508&jobAge=2&experience=12'}\n",
    "\n",
    "print(urls)\n",
    "\n",
    "# current directory\n",
    "cwd = os.getcwd()+\"/\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pwd_ip import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b863cce8-df94-4d79-8523-c3940b7b280a",
   "metadata": {},
   "source": [
    "## open chrome and get jobs tiles from the url "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d732a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to login into naurki \n",
    "\n",
    "def login_naukri(name='',password=''):\n",
    "    driver = webdriver.Chrome()\n",
    "    url = 'https://www.naukri.com/'\n",
    "    driver.get(url)\n",
    "\n",
    "    if not name and not password:  \n",
    "        time.sleep(10)\n",
    "        print('continue without login')\n",
    "        return driver\n",
    "      \n",
    "    login_button = WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.ID, 'login_Layer')))\n",
    "\n",
    "    login_button.click()\n",
    "\n",
    "    time.sleep(4)\n",
    "\n",
    "    input_fields = driver.find_elements(By.TAG_NAME, 'input')\n",
    "\n",
    "    if input_fields:\n",
    "        input_fields[0].send_keys(name)\n",
    "        input_fields[1].send_keys(password)\n",
    "\n",
    "    time.sleep(1)\n",
    "\n",
    "    submit_buttons = driver.find_elements(By.CLASS_NAME, 'loginButton')\n",
    "    # driver.execute_script(\"arguments[0].click()\",submit_button)\n",
    "    submit_buttons[0].click()\n",
    "    \n",
    "    time.sleep(2)\n",
    "\n",
    "    errors = driver.find_elements(By.CLASS_NAME, 'server-err')\n",
    "    if len(errors) > 0:\n",
    "        print('login --> failed')\n",
    "    else:\n",
    "        print('login --> success')\n",
    "\n",
    "    time.sleep(10)\n",
    "    return driver\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526857d5-86ec-45e2-9a7e-1b781fdb3926",
   "metadata": {},
   "source": [
    "### get the data - job_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f345e79-7321-4948-aae0-cf447f36ba71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_job_1():\n",
    "    \n",
    "    driver = login_naukri(job1_name,job1_password)\n",
    "\n",
    "    tiles_db = {}\n",
    "\n",
    "\n",
    "    for url_id, url in urls.items():\n",
    "        driver.get(url)\n",
    "        wait()\n",
    "        state = 'start'    \n",
    "        tiles = []\n",
    "        while state!='stop':\n",
    "            scroll(driver)\n",
    "            wait()\n",
    "            tile = collect_data(driver,tile_class='srp-jobtuple-wrapper')\n",
    "            if tile:\n",
    "                tiles = tiles + tile\n",
    "            wait()\n",
    "            state = click_next(driver,'styles_btn-secondary__2AsIP')\n",
    "            \n",
    "        t_count = 0\n",
    "        for tile in tiles:        \n",
    "            key = url_id+'--tile_'+str(t_count)\n",
    "            tiles_db[key] = str(tile)\n",
    "            t_count+=1\n",
    "\n",
    "    driver.close()\n",
    "\n",
    "    write_json(tiles_db,'job_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847177c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(cwd+\"job_1.json\"):\n",
    "    job_1_mtime = os.path.getmtime(cwd+\"job_1.json\")\n",
    "    job_1_mdate = datetime.datetime.fromtimestamp(job_1_mtime).date()\n",
    "    print(job_1_mdate)\n",
    "    print(datetime.date.today())\n",
    "\n",
    "    if not(job_1_mdate == datetime.date.today()):\n",
    "        create_job_1()\n",
    "else:\n",
    "    create_job_1()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d494a409-37de-4b88-9105-f0775b72bcd8",
   "metadata": {},
   "source": [
    "### create df - job_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4bcb47-e6e7-46db-981d-a887eebd2a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the job html file\n",
    "job_1 = read_json('job_1')\n",
    "\n",
    "#converting job elements tags\n",
    "for tile_id, tile in job_1.items():\n",
    "    job_1[tile_id] = BeautifulSoup(tile,'lxml').find('div')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0dd4d50-9993-4cc5-bab1-bd0ec8b97b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_job_1 = pd.DataFrame(columns=['job_id','company_name','job_title','job_url'])\n",
    "\n",
    "for tile_id,tile in job_1.items():\n",
    "    # extract the data from the tiles\n",
    "    \n",
    "    job_id = tile.get('data-job-id')\n",
    "    title = tile.find('a',class_='title').text.strip()\n",
    "    href = tile.find('a',class_='title')['href']\n",
    "    company = tile.find('a',class_='comp-name').text.strip()\n",
    "\n",
    "    # write the data in the dataframe\n",
    "\n",
    "    data = {\n",
    "        'job_id' : str(job_id),\n",
    "        'company_name' : company,\n",
    "        'job_title' : title,\n",
    "        'job_url' : href\n",
    "    }\n",
    "\n",
    "    df_job_1.loc[len(df_job_1)] = data\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0629b4-aaca-4ed8-8dbf-cae9f63c06a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the a columns showcasing the duplicates in a dataframe\n",
    "\n",
    "df_job_1['tile_duplicates'] = df_job_1.groupby(df_job_1.columns.tolist()).transform('size')\n",
    "df_job_1 = df_job_1.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab70bb58-0ae0-4301-9309-b785e9f639d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_job_1.to_excel(cwd+'job.xlsx',index=False)\n",
    "df_job_1.to_csv(cwd+'job_1.csv',index=False)\n",
    "print(f'total rows = {len(df_job_1)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336e84d8-ac79-4a65-94a8-8ac1822b2b97",
   "metadata": {},
   "source": [
    "## open chrome and get additional information for each jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415ae115-15e0-49bd-bfd1-5776c3d6fe94",
   "metadata": {},
   "source": [
    "### get the data - job_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497991d1-5440-4b7b-927a-20e9b0f790e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_job_1 = pd.read_csv(cwd+'job_1.csv')\n",
    "df_job_1['job_id'] = df_job_1['job_id'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8dbbbb-c232-42b4-9eb5-ddbb1b84cd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = login_naukri(job2_name,job2_password) \n",
    "\n",
    "tiles = read_json('job_2')\n",
    "count = 0\n",
    "jobs_to_be_processed = len(job_1)\n",
    "\n",
    "# for index,job_row in df_job_1.tail().iterrows():\n",
    "for index,job_row in df_job_1.iterrows():\n",
    "    job_id = str(job_row['job_id'])\n",
    "    job_url = job_row['job_url']\n",
    "    \n",
    "\n",
    "    if job_id in tiles:\n",
    "        print(f'{job_id} already in database')\n",
    "        continue\n",
    "    \n",
    "    driver.get(job_url)\n",
    "    wait()\n",
    "    scroll(driver)\n",
    "    wait()\n",
    "    tile = collect_data(driver,tile_class='styles_left-section-container__btAcB')\n",
    "    if tile:\n",
    "        tiles[job_id] = str(tile[0])\n",
    "    # tiles = tiles + [(job_id,tile[0])]\n",
    "    wait()\n",
    "    count+=1\n",
    "    if count >=40:\n",
    "        with open(cwd+'job_2.json', 'w') as json_file:\n",
    "            json.dump(tiles, json_file)\n",
    "        count = 0\n",
    "        tiles = read_json('job_2')\n",
    "        print(f'Percentage of files processed = {index/jobs_to_be_processed:.1%}')\n",
    "    print(f'{index} out of {jobs_to_be_processed} (done)')    \n",
    "\n",
    "driver.close()\n",
    "\n",
    "# writing the tiles result in a file\n",
    "with open(cwd+'job_2.json', 'w') as json_file:\n",
    "    json.dump(tiles, json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c327c66d-1485-40d2-8e98-8c7b51221674",
   "metadata": {},
   "source": [
    "### create df - job_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b905289b-acdf-4473-bbe1-ed37dacc20de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the job html file\n",
    "with open(cwd + 'job_2.json', 'r') as json_file:\n",
    "    job_2 = json.load(json_file)\n",
    "\n",
    "# changing the types of elements(tile) corresponding to job id in json dictionary\n",
    "for job_id,tile in job_2.items():\n",
    "    job_2[job_id] = BeautifulSoup(tile,'lxml').find('div')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69365c6c-3086-474e-800e-87f1c379387c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # creating a dataframe to find_out columns keys\n",
    "# tile = job_2['130225000057']\n",
    "# dict_tile, df_tile = extract_tag_data_to_dict_and_df(tile)\n",
    "# df_tile.to_csv(cwd+'df_tile.csv')\n",
    "# dict_tile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bee4526-cf97-4230-897f-51f16ae7027f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary of columns_name and \"keys to lookup in dictionary of tile\" \n",
    "\n",
    "column_find_via = {\n",
    "    # 'job_title': (\"['styles_jhc__jd-top-head__MFoZl']\", 'header', 'text'),\n",
    "    # 'company_name':\t(\"['styles_jd-header-comp-name__MvqAI']\", 'a', 'text'),\n",
    "    'company_rating' : \t(\"['styles_amb-rating__4UyFL']\", 'span', 'text'),\n",
    "    'job_experience' : \t(\"['styles_jhc__exp__k_giM']\", 'div', 'text'),\n",
    "    'job_location':\t(\"['styles_jhc__loc___Du2H']\", 'div', 'text'),\n",
    "    'remote':(\"['styles_jhc__wfhmode-link__aHmrK']\",'a','text'),\n",
    "    'salary': (\"['ni-icon-salary']\", 'span', 'text'),\n",
    "    'keywords':\t(\"['styles_chip__7YCfG', 'styles_clickable__dUW8S']\", 'span', 'text'),\n",
    "    'keywords_2':(\"['styles_chip__7YCfG', 'styles_non-clickable__RM_KJ']\", 'span', 'text'),\n",
    "    'keywords_3':(\"['styles_chip__7YCfG', 'styles_clickable__dUW8S']\",'a','text'),\n",
    "    'apply_on_company_site': (\"['styles_company-site-button__C_2YK', 'company-site-button']\", 'button', 'text'),\n",
    "    'easy_apply':(\"['styles_apply-button__uJI3A', 'apply-button']\", 'button', 'text')\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbc2de7-e0dd-44c9-ba1c-b74684d7a2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a dataframe for job_2\n",
    "\n",
    "df_job_2 = pd.DataFrame(columns=['job_id']+list(column_find_via.keys())+['keyskills_match'])\n",
    "\n",
    "# populating dataframe for job_2\n",
    "\n",
    "for job_id,tile in job_2.items():\n",
    "    data = {\n",
    "        'job_id':str(job_id)\n",
    "    }\n",
    "    \n",
    "    dict_tile, df_tile = extract_tag_data_to_dict_and_df(tile)\n",
    "    for col in column_find_via:\n",
    "        data[col] = dict_tile.get(column_find_via[col])\n",
    "    \n",
    "    key_skills_match_div = ''\n",
    "    for div in tile.find_all(attrs={'class':'styles_MS__details__iS7mj'}):\n",
    "        if \"keyskills\" in div.get_text().lower():\n",
    "            key_skills_match_div = div.find(\"i\").get('class')[0]\n",
    "    data['keyskills_match'] = key_skills_match_div\n",
    "\n",
    "    df_job_2.loc[len(df_job_2)] = data\n",
    "\n",
    "df_job_2 = df_job_1.merge(df_job_2,on='job_id',how='left')   \n",
    "\n",
    "# df_job_2.to_excel(cwd+'job.xlsx',index=False)\n",
    "df_job_2.to_csv(cwd+'job_2.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de4e85c-8ae3-421d-9501-5ecd3e401192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering jobs\n",
    "\n",
    "df_job_3 = pd.read_csv(cwd+'job_2.csv')\n",
    "\n",
    "## write filtering criteria here\n",
    "\n",
    "df_job_3.to_csv(cwd+'job_3.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4681873-a23a-48e8-befe-3d0aad3d0fc7",
   "metadata": {},
   "source": [
    "### modifying the job_data to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b62256-2871-490a-9d9a-bd493f6b19f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the dataframe\n",
    "\n",
    "df_job_3 = pd.read_csv(cwd+'job_3.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545c5f89-6dca-4c58-868f-83a7cbf2d7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keyword score\n",
    "\n",
    "# keyword_terms = ['advanced analytics','manager','analyst','analytics','data','visualization','python','reporting','business intelligence','project management','product management','business analysis']\n",
    "\n",
    "keyword_terms = [\n",
    "    \"SQL\",\n",
    "    \"Python\",\n",
    "    \"Analytics\",\n",
    "    \"Business Analysis\",\n",
    "    \"Data Visualization\",\n",
    "    \"Reporting\",\n",
    "    \"Insights\",\n",
    "    \"Stakeholder Management\",\n",
    "    \"Product Management\",\n",
    "    \"Program Management\",\n",
    "    \"Project Management\",\n",
    "    \"Advanced Excel\",\n",
    "    \"Data Governance\",\n",
    "    \"Data Management\",\n",
    "    \"Business Strategy\",\n",
    "    \"strategy\",\n",
    "    \"analyst\",\n",
    "    \"data\",\n",
    "    \"manager\",\n",
    "    \"report\",\n",
    "    \"agile\"\n",
    "]\n",
    "\n",
    "\n",
    "def calculate_score(keywords):\n",
    "    score = 0\n",
    "    if pd.isna(keywords):\n",
    "        return 0\n",
    "    for k in keyword_terms:\n",
    "        score += keywords.lower().count(k.lower())\n",
    "    return score\n",
    "\n",
    "df_job_3['keyword_score'] = df_job_3['keywords_3'].apply(calculate_score)+df_job_3['keywords'].apply(calculate_score) + 2*df_job_3['job_title'].apply(calculate_score) + df_job_3['keywords_2'].apply(calculate_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09f2866-c01a-46e0-812e-65423dc10fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "location_score_db = {\n",
    "    'remote':1,\n",
    "    'mumbai':2,\n",
    "    'delhi':3,\n",
    "    'noida':3,\n",
    "    'gurugram':3,\n",
    "    'gurgaon':3,\n",
    "    'ncr':3,\n",
    "    'bangalore':4,\n",
    "    'bengaluru':4,\n",
    "    'pune':5,\n",
    "    'hyderabad':6\n",
    "}\n",
    "\n",
    "def location_score(location):\n",
    "    if pd.isna(location):\n",
    "        return 10\n",
    "    score=10\n",
    "    for loc,score in location_score_db.items():\n",
    "        if loc.lower() in location.lower():\n",
    "            return score\n",
    "\n",
    "    return score\n",
    "\n",
    "df_job_3['location_score'] = df_job_3['job_location'].apply(location_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3791d19-246f-4f8b-aeff-ffdd202b7955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorting the dataframe\n",
    "\n",
    "df_job_3 = df_job_3.sort_values(by=['keyword_score','company_rating','location_score','easy_apply','company_name'], ascending=[False,False, True,False,True])\n",
    "df_job_3['display_text'] = df_job_3['keyword_score'].astype(str).str.cat(df_job_3['company_rating'].astype(str), sep=' | ',na_rep='NA').str.cat(df_job_3['salary'].astype(str), sep=' | ',na_rep = 'NA').str.cat(df_job_3['job_title'].astype(str), sep=' | ',na_rep = 'NA').str.cat(df_job_3['company_name'].astype(str), sep=' | ',na_rep = 'NA').str.cat(df_job_3['job_location'].astype(str),sep=' | ',na_rep = 'NA')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_job_3.rename(columns={'job_url':'url',\n",
    "                         'job_id':'id'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_job_3.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "required_columns = ['id', 'url',\n",
    "                    'company_name','company_rating',\n",
    "                    'job_title','job_experience', 'salary',\n",
    "                    'job_location', 'remote',\n",
    "                    'keywords',\n",
    "                    'apply_on_company_site','easy_apply',\n",
    "                    'keyskills_match', 'keyword_score', 'location_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_job_3 = df_job_3[required_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_column(df,condition,col_to_modify,previous_value,new_value):\n",
    "    df = df.copy()\n",
    "    if col_to_modify not in df.columns:\n",
    "        df[col_to_modify] = previous_value\n",
    "\n",
    "    filtered_df = df.query(f'{condition} and {col_to_modify}==\"{previous_value}\"')\n",
    "    df.loc[filtered_df.index,col_to_modify]= new_value\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## category assignment\n",
    "\n",
    "max_keyword_score = df_job_3[ 'keyword_score']. max()\n",
    "category_conditions = {\n",
    "\n",
    "'A1_skill_match_delhi': \"(location_score == 3) and (keyskills_match == 'ni-icon-check_circle') and (easy_apply == 'Apply')\",\n",
    "\n",
    "'A2_skill_match_remote' : \"((location_score == 1) or (remote =='Remote')) and (keyskills_match == 'ni-icon-check_circle') and (easy_apply == 'Apply' )\",\n",
    "\n",
    "'A3_score_match':\"keyword_score > @max_keyword_score - 10 and (easy_apply == 'Apply')\",\n",
    "\n",
    "\n",
    "'A4_skill_match_good_company': \"(company_rating > 3.5 or company_rating.isnull()) and (easy_apply ==  'Apply') and (keyskills_match == 'ni-icon-check_circle')\",\n",
    "\n",
    "'A5_remaining_easy_apply':\"(easy_apply ==  'Apply')\",\n",
    "\n",
    "\n",
    "'B1_skill_match_remote_delhi_good_company': \"( (location_score == 3) or (location_score == 1) or (remote =='Remote') ) and (keyskills_match == 'ni-icon-check_circle') and (apply_on_company_site == 'Apply on company site') and ( company_rating > 3.5 or company_rating.isnull() )\"\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cat,condition in category_conditions.items():\n",
    "    df_job_3 = modify_column(df_job_3,condition,'category','ZZZ',cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_job_3.sort_values(by=['category'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_job_3.to_json('n.json',orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
